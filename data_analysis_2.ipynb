{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis and Data Science\n",
    "## Be curious about the methods\n",
    "\n",
    "By: Caroline Labelle\n",
    "<br>For: BCM6065-65\n",
    "\n",
    "<br>\n",
    "Date: July 4th 2022\n",
    "\n",
    "<hr style=\"border:1px solid black\"> </hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and importing Python libraries\n",
    "\n",
    "Befor using (or importing) a library in Python, you first need to install it!. This step only need to be done once for each library: once istalled, you will have access to all the libraries from your coding environment.\n",
    "\n",
    "Ressource: https://pip.pypa.io/en/stable/user_guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Installing scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import sklearn\n",
    "import sklearn.decomposition, sklearn.cluster\n",
    "\n",
    "### Import scipy\n",
    "import scipy as sp\n",
    "\n",
    "### Import pandas, numpy, seaborn and matplotlib.pyplot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(9, 4)})\n",
    "sns.set_theme(context=\"notebook\", style=\"white\", palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be mainly using the <code>sklearn</code> and <code>scipy</code> libraries to implement and use various data analysis methods.\n",
    "\n",
    "scikit-learn ressource: https://scikit-learn.org/stable/index.html\n",
    "Scipy ressource: https://docs.scipy.org/doc/scipy/reference/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Once we've explored our dataset and have a better undesrtanding of what it contains, we can start to analyse it! Before applying any kind of methods, we must first establish \"what we want to know\".\n",
    "\n",
    "Do we want to **fit** our data to a model and/or assess if there is a **correlation** between variables? Do we want to **decompose** our dataset and/or identify **clusters**?\n",
    "\n",
    "Once we establish \"what we want to know\", we need to define \"how we'll do it\"! There exist different methods for a single task... We must be curious about the methods and use the one that is the most appropriate to our *context*! \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and Curve fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression with Scipy\n",
    "\n",
    "<code>scipy.stats.linregress(x, y, alternative='two-sided')</code>\n",
    "\n",
    "* **x, y**: sets of measurements\n",
    "* **alternative='two-sided'**: the alternative hypothesis ($H_{a}$) is that the slope of the regression line is nonzero\n",
    "\n",
    "Here, the null hypothesis ($H_{0}$) is that the slope is zero.\n",
    "\n",
    "Ressource: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html#scipy.stats.linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import and clean the penguins dataset\n",
    "data_penguin = \n",
    "\n",
    "### Re-index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot a pairwise comparison figure with seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flipper length and body mass seems to be highly corrolated. We want to confirm this by applying a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do a linear regression\n",
    "reg = sp.stats.linregress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look a the results\n",
    "reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <code>rvalue</code> represents the correlation coefficient. The Pearson's correlation coefficient measures the linear relationship between two variables.\n",
    "\n",
    "**Important**: a correlation of 0 does not imply no correlation!\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice [10 points]\n",
    "Select a pair of variables (other than Flipper length vs. body mass). \n",
    "1. Apply a linear regression for each species;\n",
    "2. Plot the datapoints and a the linear fit obtained for each flower type (you plot three plots independently).\n",
    "\n",
    "Which species type has the highest correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curve fitting with Scipy\n",
    "\n",
    "<code>scipy.optimize.curve_fit(f, x, y)</code>\n",
    "\n",
    "* **f**: model function such that $f(x, ...)$\n",
    "* **x, y**: sets of measurements\n",
    "\n",
    "\n",
    "Ressource: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html#scipy.optimize.curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Linear of regression with curve_fit\n",
    "### Define the model f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do linear curve fitting of petal length vs. petal width\n",
    "param, cov = sp.optimize.curve_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Look at results and compare to linregress results\n",
    "### Param:\n",
    "\n",
    "### R Coef:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>numpy.corrcoef()</code> returns the Pearson correlation coefficient matrix of the variables.\n",
    "\n",
    "Ressource: https://numpy.org/doc/stable/reference/generated/numpy.corrcoef.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice [20 points]\n",
    "Import the dose-response dataset. We tested various concentrations for 12 different drugs on AML3 cells. The dataset contains the viability responses. \n",
    "\n",
    "* Row names are the $log_{10}$ concentration\n",
    "* Column names are the various drugs we tested\n",
    "* Cells values are the $\\%$ viability of AML3 for a given concentration of a given compound.\n",
    "\n",
    "\n",
    "**Find which drug has the smallest IC$_{50}$ and which drug has the smallest minimal viabiality.**\n",
    "\n",
    "It is common practice to model dose-response data with the log-logistic model:<br>\n",
    "$$f(x) = Min + \\frac{Max - Min}{1 + 10 ^ {x - IC_{50}}} $$<br>\n",
    "\n",
    "where $x$ is a dose and $f(x)$ the $\\%$ viability for that given dose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define a function for the log-logistic model\n",
    "### np.power() : https://numpy.org/doc/stable/reference/generated/numpy.power.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do curve fitting (non-linear regression) on viability for 17-AAG\n",
    "### Plot the resulting curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do curve fitting (non-linear regression) on viability on each drug\n",
    "### Find the smallest predicted IC50 and smallest predicted Min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS [5 points]\n",
    "**Is there a linear correlation between the IC50 and Min values?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do curve fitting (non-linear regression) on viability on each drug\n",
    "### Save predicted IC50 and predicted Min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculte linear regression for IC50 vs. Min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot IC50 vs. Min values with linear fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Dimensionality reduction\n",
    "\n",
    "Dimensionality reduction allows use to reduce th enumber of random variables to consider. It is primarly useful for visualisation purpose and to increase the efficiency of other analysis methods (eg. clustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA with scikit learn\n",
    "\n",
    "**PCA** = Principal Component Analysis\n",
    "\n",
    "PCA is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set. *We are trading a little bit of accuracy for simplicity*.\n",
    "\n",
    "Ressource: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Show the penguins dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the data we need for the PCA\n",
    "X = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initiate the PCA and apply it to our data\n",
    "pca = sklearn.decomposition.PCA()\n",
    "pca.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new scaler\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "\n",
    "# Learn the pattern from the input data\n",
    "scaler.fit()\n",
    "\n",
    "#Apply the pattern\n",
    "X_scaled = scaler.transform() \n",
    "\n",
    "### Initiate the PCA and apply it to the scaled data\n",
    "pca = sklearn.decomposition.PCA()\n",
    "pca.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal components are new variables that are constructed as linear combinations or mixtures of the initial variables. These combinations are done in such a way that the new variables (i.e., principal components) are uncorrelated and most of the information within the initial variables is squeezed or compressed into the first components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attributes of the pca object:**\n",
    "\n",
    "<code>pca.n_components_</code>: estimated number of components\n",
    "\n",
    "<code>pca.n_features_</code>: number of features in the training data\n",
    "\n",
    "<code>pca.n_samples_</code>: number of samples in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get estimated number of components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get number of features in the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get number of samples in the training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA tries to put maximum possible information in the first component, then maximum remaining information in the second and so on.\n",
    "\n",
    "<code>pca.explained_variance_</code>: amount of variance explained by each of the selected components\n",
    "\n",
    "<code>pca.explained_variance_ratio_</code>: Percentage of variance explained by each of the selected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the explained_variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the explained_variance_ratio_\n",
    "## What do you notice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the explained_variance_ratio_\n",
    "## Bar plot: https://seaborn.pydata.org/generated/seaborn.barplot.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply the dimensionality reduction to our data\n",
    "X_reduce = pca.fit_transform()\n",
    "\n",
    "### Make a dataframe and add labelling columns\n",
    "reduce_penguin = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the results of the reduction\n",
    "### How many PC should we use?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can be said of the above plot?\n",
    "\n",
    "Remember, the principal components are less interpretable and don’t have any real meaning since they are constructed as linear combinations of the initial variables..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ressources to learn more about PCA\n",
    "* https://builtin.com/data-science/step-step-explanation-principal-component-analysis\n",
    "* https://www.youtube.com/watch?v=HMOI_lkzW08\n",
    "* https://www.youtube.com/watch?v=FgakZw6K1QQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice  [20 points]\n",
    "\n",
    "Import the Wisconsin breast cancer dataset. We are interested in identifying tumors types.\n",
    "\n",
    "Can you identify a pair of variables that seems to be linearly correlated? \n",
    "* **What is the r coefficient?** \n",
    "* **Do you find different r coefficient for the different tumors type?**\n",
    "\n",
    "Apply a PCA on the dataset. Make sur to plot the percent of explained variance, and the results of the reduction apply to data.\n",
    "* **What can you conclude regarding the tumors types?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do a pairplot of all the variable pairing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do linear regression on pair of variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do linear regression on pair of variables based on tumour type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the data we need for the PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initiate de PCA and apply it to our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the explained_variance_ratio_\n",
    "## What do you notice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply the dimensionality reduction to our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the results of the reduction\n",
    "### How many PC should we use?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Clustering\n",
    "\n",
    "Clustering methodologies allow us to automatically group similar object into sets. There exist many clustering methodologies!\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-means with scikit-learn\n",
    "\n",
    "The KMeans algorithm clusters data by trying to separate samples in $n$ groups of equal variance, minimizing a criterion known as **within-cluster sum-of-squares**.\n",
    "\n",
    "This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields.\n",
    "\n",
    "At a glance, the k-means algorithm divides a set of $N$ samples $X$ into $K$ disjoint clusters $C$, each described by the mean $\\mu_{j}$ of the samples in the cluster $C_{j}$. \n",
    "\n",
    "*In very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called “curse of dimensionality”). Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to k-means clustering can alleviate this problem and speed up the computations.*\n",
    "\n",
    "Ressource: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the k-means algorithm to the first two principal component of the Iris dataset. **Are we able to cluster the datapoint based on their flower type?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the data\n",
    "X = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initiate the k-means alogirthm\n",
    "### How many cluster should we use?\n",
    "kmean = sklearn.cluster.KMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply kmeans to our data\n",
    "kmeans_X = kmean.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attributes of the kmeans object:**\n",
    "\n",
    "<code>kmeans.cluster_centers_</code>: coordinates of cluster centers\n",
    "\n",
    "<code>kmeans.labels_</code>: labels of each point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get centroids coordinate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get datapoints labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add the cluster column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the cluster and the labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not always easy to define the number of cluster to use!\n",
    "\n",
    "The most common approach for deciding the value of $K$ is the so-called elbow method. It involves running the algorithm multiple times over a loop, with an increasing number of cluster choice and then plotting a clustering score as a function of the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>inertia_</code>: sum of squared distances of samples to their closest cluster center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the inertia of our initial kmean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run kmeans for various k values\n",
    "### initiate empty list\n",
    "\n",
    "### Create for loop for K from 1 to 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot K vs. interti\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice [10 points]\n",
    "\n",
    "Go back the Wisconsin breast cancer dataset. **Can you cluster the datapoint in $K$ clusters?**\n",
    "\n",
    "You must first decide which value to give $K$. Make sure to leave some trace of your tought process...<br>\n",
    "Apply the kmean algorithme to the data and plot the results. **Can you find a link between one of the dataset features and the clusters?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bioinfo 2)",
   "language": "python",
   "name": "bioinfo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
